# MAMBAQUANT: QUANTIZING THE MAMBA FAMILY WITH VARIANCE ALIGNED ROTATION METHODS

![MAMBAQUANT Logo](https://github.com/yourusername/mambaquant/blob/main/assets/logo.png)

## Overview

**MAMBAQUANT**, a novel post-training quantization framework tailored for Mamba models, addressing key challenges such as outliers and variance inconsistencies. MambaQuant achieves less than 1% accuracy loss in quantizing weights and activations to 8-bit for various Mamba-based tasks, marking the first comprehensive PTQ design for this family.

This repository accompanies our ICLR 2024 paper titled **"MAMBAQUANT: QUANTIZING THE MAMBA FAMILY WITH VARIANCE ALIGNED ROTATION METHODS"**. 

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Contributing](#contributing)
- [Citation](#citation)
- [License](#license)
- [Contact](#contact)
- [Acknowledgements](#acknowledgements)

## Features

- **KLT-Enhanced Rotation**: 
- **Smooth-Fused Rotation**: 
- **Performance Optimization**: 

## Installation

*Coming Soon.*

Detailed instructions for setting up the MQUANT framework will be provided once the code is released. Stay tuned for updates!

## Usage

*Coming Soon.*

Comprehensive usage examples and tutorials will be available with the code release to help you get started with MAMBAQUANT effortlessly.

## Contributing

We welcome contributions from the research and development community! Whether you're interested in improving the existing features, adding new functionalities, or reporting issues, your input is invaluable.

## Citation

If you find MAMBAQUANT useful in your research, please consider citing our paper:

```bibtex
@inproceedings{mambaquant2024,
  title={MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods},
  ...
}
