vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2 w8a8 method_2 -------------------------------------------------
Not using distributed mode
batch_size: 256
epochs: 300
bce_loss: False
unscale_lr: False
model: vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2
input_size: 224
drop: 0.0
drop_path: 0.1
model_ema: True
model_ema_decay: 0.99996
model_ema_force_cpu: False
opt: adamw
opt_eps: 1e-08
opt_betas: None
clip_grad: None
momentum: 0.9
weight_decay: 0.05
sched: cosine
lr: 0.0005
lr_noise: None
lr_noise_pct: 0.67
lr_noise_std: 1.0
warmup_lr: 1e-06
min_lr: 1e-05
decay_epochs: 30
warmup_epochs: 5
cooldown_epochs: 10
patience_epochs: 10
decay_rate: 0.1
color_jitter: 0.3
aa: rand-m9-mstd0.5-inc1
smoothing: 0.1
train_interpolation: bicubic
repeated_aug: True
train_mode: True
ThreeAugment: False
src: False
reprob: 0.25
remode: pixel
recount: 1
resplit: False
mixup: 0.8
cutmix: 1.0
cutmix_minmax: None
mixup_prob: 1.0
mixup_switch_prob: 0.5
mixup_mode: batch
teacher_model: regnety_160
teacher_path: 
distillation_type: none
distillation_alpha: 0.5
distillation_tau: 1.0
cosub: False
finetune: 
attn_only: False
data_path: /data01/datasets/imagenet
data_set: IMNET
inat_category: name
output_dir: 
device: cuda
seed: 0
resume: ./saved_checkpoint/vim_t+_midclstok_ft_78p3acc.pth
start_epoch: 0
eval: True
eval_crop_ratio: 0.875
dist_eval: False
num_workers: 10
pin_mem: True
distributed: False
world_size: 1
dist_url: env://
if_amp: True
if_continue_inf: False
if_nan2num: False
if_random_cls_token_position: False
if_random_token_rank: False
local_rank: 0
use_vim_torch: True
static_quant: True
observe: percentile
quant_weight: True
quant_act: True
a_bit: 8
w_bit: 8
use_smoothquant: False
use_gptq: False
use_hadmard: True
use_S1: False
use_S2: True
use_S3: True
use_S4: False
use_S5: False
use_S7: False
use_hadmard_R1: True
use_hadmard_R2: True
use_hadmard_R3: False
use_hadmard_R4: True
use_hadmard_R5: True
use_hadmard_R6: False
use_reduce_mean: True
use_pertoken: True
use_split: True
use_klt: True
generate_klt: False
use_perkernel: False
w_perchannel: True
fake_online_hadamard: False
analyse_and_plot: False
use_adaround: False
adaround_iter: 200
b_start: 20
b_end: 2
warmup: 0.2
Creating model: vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2
number of params: 7250344
Test:  [0/1]  eta: 0:00:43  loss: 0.7967 (0.7967)  acc1: 80.8000 (80.8000)  acc5: 96.8000 (96.8000)  time: 43.8718  data: 2.9655  max mem: 30495
Test: Total time: 0:00:43 (43.9602 s / it)
* Acc@1 80.800 Acc@5 96.800 loss 0.797
Fp Accuracy of the network on the 50000 test images: 80.8%
Test:  [  0/131]  eta: 1:57:47  loss: 0.8266 (0.8266)  acc1: 84.8958 (84.8958)  acc5: 97.1354 (97.1354)  time: 53.9524  data: 6.7816  max mem: 47733
Test:  [  5/131]  eta: 0:46:41  loss: 0.7633 (0.7819)  acc1: 84.8958 (85.6337)  acc5: 97.1354 (97.0052)  time: 22.2348  data: 1.1306  max mem: 47754
Test:  [ 10/131]  eta: 0:39:01  loss: 0.8266 (0.8808)  acc1: 83.3333 (82.2206)  acc5: 96.6146 (96.4725)  time: 19.3476  data: 0.6168  max mem: 47754
Test:  [ 15/131]  eta: 0:35:18  loss: 0.8133 (0.8271)  acc1: 84.8958 (84.2448)  acc5: 96.6146 (96.5983)  time: 18.2645  data: 0.4242  max mem: 47754
Test:  [ 20/131]  eta: 0:32:44  loss: 0.7769 (0.7920)  acc1: 86.1979 (85.3051)  acc5: 96.6146 (96.7758)  time: 15.8830  data: 0.0003  max mem: 47754
Test:  [ 25/131]  eta: 0:30:38  loss: 0.8290 (0.8464)  acc1: 83.3333 (83.8642)  acc5: 96.3542 (96.3341)  time: 15.8802  data: 0.0003  max mem: 47754
Test:  [ 30/131]  eta: 0:28:48  loss: 0.8330 (0.8607)  acc1: 83.5938 (83.4929)  acc5: 96.3542 (96.2786)  time: 15.8795  data: 0.0003  max mem: 47754
Test:  [ 35/131]  eta: 0:27:06  loss: 0.8932 (0.8588)  acc1: 81.5104 (83.1959)  acc5: 96.3542 (96.4627)  time: 15.8766  data: 0.0003  max mem: 47754
Test:  [ 40/131]  eta: 0:25:29  loss: 0.9129 (0.8664)  acc1: 79.9479 (83.0348)  acc5: 96.3542 (96.5003)  time: 15.8758  data: 0.0003  max mem: 47754
Test:  [ 45/131]  eta: 0:23:56  loss: 0.8330 (0.8455)  acc1: 83.8542 (83.6107)  acc5: 97.1354 (96.6486)  time: 15.8730  data: 0.0003  max mem: 47754
Test:  [ 50/131]  eta: 0:22:26  loss: 0.8216 (0.8496)  acc1: 83.8542 (83.5223)  acc5: 97.3958 (96.6554)  time: 15.8708  data: 0.0003  max mem: 47754
Test:  [ 55/131]  eta: 0:20:58  loss: 0.8216 (0.8721)  acc1: 83.8542 (82.9474)  acc5: 96.6146 (96.3774)  time: 15.8732  data: 0.0003  max mem: 47754
Test:  [ 60/131]  eta: 0:19:31  loss: 1.0105 (0.9086)  acc1: 77.6042 (82.1465)  acc5: 94.5312 (95.9614)  time: 15.8745  data: 0.0003  max mem: 47754
Test:  [ 65/131]  eta: 0:18:05  loss: 1.2109 (0.9549)  acc1: 74.7396 (81.0172)  acc5: 91.6667 (95.4388)  time: 15.8759  data: 0.0003  max mem: 47754
Test:  [ 70/131]  eta: 0:16:41  loss: 1.2893 (0.9753)  acc1: 73.4375 (80.4908)  acc5: 91.4062 (95.2245)  time: 15.8753  data: 0.0003  max mem: 47754
Test:  [ 75/131]  eta: 0:15:17  loss: 1.2834 (0.9755)  acc1: 73.9583 (80.5750)  acc5: 91.1458 (95.1720)  time: 15.8739  data: 0.0003  max mem: 47754
Test:  [ 80/131]  eta: 0:13:53  loss: 1.2648 (0.9955)  acc1: 74.7396 (80.1183)  acc5: 91.6667 (94.8849)  time: 15.8714  data: 0.0003  max mem: 47754
Test:  [ 85/131]  eta: 0:12:30  loss: 1.2648 (1.0205)  acc1: 74.7396 (79.5119)  acc5: 91.6667 (94.6554)  time: 15.8730  data: 0.0003  max mem: 47754
Test:  [ 90/131]  eta: 0:11:08  loss: 1.2331 (1.0340)  acc1: 75.7812 (79.1667)  acc5: 92.1875 (94.5742)  time: 15.8726  data: 0.0003  max mem: 47754
Test:  [ 95/131]  eta: 0:09:45  loss: 1.2772 (1.0445)  acc1: 72.1354 (78.9903)  acc5: 92.1875 (94.4580)  time: 15.8731  data: 0.0003  max mem: 47754
Test:  [100/131]  eta: 0:08:23  loss: 1.2928 (1.0569)  acc1: 72.1354 (78.6742)  acc5: 91.4062 (94.2837)  time: 15.8750  data: 0.0003  max mem: 47754
Test:  [105/131]  eta: 0:07:02  loss: 1.2928 (1.0674)  acc1: 74.2188 (78.4174)  acc5: 91.4062 (94.1038)  time: 15.8745  data: 0.0003  max mem: 47754
Test:  [110/131]  eta: 0:05:40  loss: 1.2981 (1.0813)  acc1: 73.6979 (78.0265)  acc5: 90.3646 (93.9799)  time: 15.8760  data: 0.0003  max mem: 47754
Test:  [115/131]  eta: 0:04:19  loss: 1.2928 (1.0881)  acc1: 72.3958 (77.8870)  acc5: 90.3646 (93.8825)  time: 15.8753  data: 0.0003  max mem: 47754
Test:  [120/131]  eta: 0:02:58  loss: 1.2633 (1.0953)  acc1: 72.3958 (77.6816)  acc5: 91.6667 (93.8447)  time: 15.8746  data: 0.0002  max mem: 47754
Test:  [125/131]  eta: 0:01:37  loss: 1.1664 (1.0906)  acc1: 73.9583 (77.7964)  acc5: 92.4479 (93.8864)  time: 15.8727  data: 0.0002  max mem: 47754
Test:  [130/131]  eta: 0:00:16  loss: 1.1554 (1.0950)  acc1: 75.0000 (77.7640)  acc5: 92.9688 (93.9220)  time: 15.2878  data: 0.0002  max mem: 47754
Test: Total time: 0:35:06 (16.0783 s / it)
* Acc@1 77.764 Acc@5 93.922 loss 1.095
Accuracy of the network on the 50000 test images: 77.8%
vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2 w8a8 method_1.5 -------------------------------------------------
Not using distributed mode
batch_size: 256
epochs: 300
bce_loss: False
unscale_lr: False
model: vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2
input_size: 224
drop: 0.0
drop_path: 0.1
model_ema: True
model_ema_decay: 0.99996
model_ema_force_cpu: False
opt: adamw
opt_eps: 1e-08
opt_betas: None
clip_grad: None
momentum: 0.9
weight_decay: 0.05
sched: cosine
lr: 0.0005
lr_noise: None
lr_noise_pct: 0.67
lr_noise_std: 1.0
warmup_lr: 1e-06
min_lr: 1e-05
decay_epochs: 30
warmup_epochs: 5
cooldown_epochs: 10
patience_epochs: 10
decay_rate: 0.1
color_jitter: 0.3
aa: rand-m9-mstd0.5-inc1
smoothing: 0.1
train_interpolation: bicubic
repeated_aug: True
train_mode: True
ThreeAugment: False
src: False
reprob: 0.25
remode: pixel
recount: 1
resplit: False
mixup: 0.8
cutmix: 1.0
cutmix_minmax: None
mixup_prob: 1.0
mixup_switch_prob: 0.5
mixup_mode: batch
teacher_model: regnety_160
teacher_path: 
distillation_type: none
distillation_alpha: 0.5
distillation_tau: 1.0
cosub: False
finetune: 
attn_only: False
data_path: /data01/datasets/imagenet
data_set: IMNET
inat_category: name
output_dir: 
device: cuda
seed: 0
resume: ./saved_checkpoint/vim_t+_midclstok_ft_78p3acc.pth
start_epoch: 0
eval: True
eval_crop_ratio: 0.875
dist_eval: False
num_workers: 10
pin_mem: True
distributed: False
world_size: 1
dist_url: env://
if_amp: True
if_continue_inf: False
if_nan2num: False
if_random_cls_token_position: False
if_random_token_rank: False
local_rank: 0
use_vim_torch: True
static_quant: True
observe: percentile
quant_weight: True
quant_act: True
a_bit: 8
w_bit: 8
use_smoothquant: False
use_gptq: False
use_hadmard: True
use_S1: False
use_S2: True
use_S3: True
use_S4: False
use_S5: False
use_S7: False
use_hadmard_R1: True
use_hadmard_R2: False
use_hadmard_R3: False
use_hadmard_R4: False
use_hadmard_R5: True
use_hadmard_R6: False
use_reduce_mean: True
use_pertoken: True
use_split: True
use_klt: True
generate_klt: False
use_perkernel: False
w_perchannel: True
fake_online_hadamard: False
analyse_and_plot: False
use_adaround: False
adaround_iter: 200
b_start: 20
b_end: 2
warmup: 0.2
Creating model: vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2
number of params: 7250344
Test:  [0/1]  eta: 0:00:42  loss: 0.7966 (0.7966)  acc1: 80.8000 (80.8000)  acc5: 96.8000 (96.8000)  time: 42.9068  data: 2.4415  max mem: 30493
Test: Total time: 0:00:42 (42.9912 s / it)
* Acc@1 80.800 Acc@5 96.800 loss 0.797
Fp Accuracy of the network on the 50000 test images: 80.8%
Test:  [  0/131]  eta: 1:50:22  loss: 0.9973 (0.9973)  acc1: 82.2917 (82.2917)  acc5: 95.5729 (95.5729)  time: 50.5565  data: 6.9797  max mem: 50976
Test:  [  5/131]  eta: 0:43:59  loss: 0.8183 (0.8797)  acc1: 83.0729 (85.3299)  acc5: 95.5729 (96.7448)  time: 20.9463  data: 1.1636  max mem: 50996
Test:  [ 10/131]  eta: 0:36:48  loss: 0.9199 (0.9673)  acc1: 83.0729 (81.7235)  acc5: 95.5729 (96.0938)  time: 18.2554  data: 0.6348  max mem: 50996
Test:  [ 15/131]  eta: 0:33:20  loss: 0.9052 (0.9037)  acc1: 83.3333 (83.7077)  acc5: 95.5729 (96.2728)  time: 17.2459  data: 0.4365  max mem: 50996
Test:  [ 20/131]  eta: 0:30:55  loss: 0.8183 (0.8575)  acc1: 86.1979 (84.7222)  acc5: 96.6146 (96.5154)  time: 15.0243  data: 0.0003  max mem: 50996
Test:  [ 25/131]  eta: 0:28:57  loss: 0.9052 (0.9071)  acc1: 83.0729 (83.0529)  acc5: 95.5729 (96.1839)  time: 15.0241  data: 0.0003  max mem: 50996
Test:  [ 30/131]  eta: 0:27:13  loss: 0.8938 (0.9198)  acc1: 83.0729 (82.7453)  acc5: 95.8333 (96.0602)  time: 15.0242  data: 0.0003  max mem: 50996
Test:  [ 35/131]  eta: 0:25:37  loss: 0.9258 (0.9161)  acc1: 80.7292 (82.6100)  acc5: 96.3542 (96.2023)  time: 15.0248  data: 0.0003  max mem: 50996
Test:  [ 40/131]  eta: 0:24:06  loss: 0.9981 (0.9252)  acc1: 79.9479 (82.4441)  acc5: 96.0938 (96.2144)  time: 15.0249  data: 0.0003  max mem: 50996
Test:  [ 45/131]  eta: 0:22:38  loss: 0.9130 (0.9049)  acc1: 83.5938 (82.9654)  acc5: 96.6146 (96.4221)  time: 15.0236  data: 0.0003  max mem: 50996
Test:  [ 50/131]  eta: 0:21:13  loss: 0.9108 (0.9045)  acc1: 83.0729 (82.8431)  acc5: 97.1354 (96.4359)  time: 15.0220  data: 0.0003  max mem: 50996
Test:  [ 55/131]  eta: 0:19:49  loss: 0.9108 (0.9270)  acc1: 82.8125 (82.3103)  acc5: 96.3542 (96.1170)  time: 15.0204  data: 0.0003  max mem: 50996
Test:  [ 60/131]  eta: 0:18:28  loss: 1.0125 (0.9628)  acc1: 76.8229 (81.3439)  acc5: 94.7917 (95.6583)  time: 15.0204  data: 0.0003  max mem: 50996
Test:  [ 65/131]  eta: 0:17:07  loss: 1.2355 (1.0115)  acc1: 73.4375 (80.2241)  acc5: 92.1875 (95.0560)  time: 15.0216  data: 0.0003  max mem: 50996
Test:  [ 70/131]  eta: 0:15:46  loss: 1.3509 (1.0352)  acc1: 71.6146 (79.5481)  acc5: 90.8854 (94.8357)  time: 15.0218  data: 0.0003  max mem: 50996
Test:  [ 75/131]  eta: 0:14:27  loss: 1.3443 (1.0366)  acc1: 71.8750 (79.6018)  acc5: 90.8854 (94.7574)  time: 15.0214  data: 0.0003  max mem: 50996
Test:  [ 80/131]  eta: 0:13:08  loss: 1.3471 (1.0588)  acc1: 70.3125 (79.0156)  acc5: 90.6250 (94.4573)  time: 15.0225  data: 0.0003  max mem: 50996
Test:  [ 85/131]  eta: 0:11:50  loss: 1.3271 (1.0841)  acc1: 70.3125 (78.4672)  acc5: 91.4062 (94.1770)  time: 15.0231  data: 0.0003  max mem: 50996
Test:  [ 90/131]  eta: 0:10:31  loss: 1.2757 (1.0990)  acc1: 73.1771 (78.1536)  acc5: 90.6250 (94.0104)  time: 15.0251  data: 0.0003  max mem: 50996
Test:  [ 95/131]  eta: 0:09:14  loss: 1.4262 (1.1107)  acc1: 70.5729 (77.9405)  acc5: 90.3646 (93.8748)  time: 15.0254  data: 0.0003  max mem: 50996
Test:  [100/131]  eta: 0:07:56  loss: 1.3578 (1.1227)  acc1: 70.8333 (77.6661)  acc5: 90.3646 (93.7036)  time: 15.0244  data: 0.0003  max mem: 50996
Test:  [105/131]  eta: 0:06:39  loss: 1.3578 (1.1347)  acc1: 73.1771 (77.3683)  acc5: 90.3646 (93.5584)  time: 15.0258  data: 0.0003  max mem: 50996
Test:  [110/131]  eta: 0:05:22  loss: 1.3776 (1.1503)  acc1: 70.8333 (77.0130)  acc5: 90.3646 (93.3981)  time: 15.0234  data: 0.0003  max mem: 50996
Test:  [115/131]  eta: 0:04:05  loss: 1.3578 (1.1583)  acc1: 70.8333 (76.8543)  acc5: 90.3646 (93.2516)  time: 15.0234  data: 0.0003  max mem: 50996
Test:  [120/131]  eta: 0:02:48  loss: 1.3439 (1.1673)  acc1: 70.8333 (76.6034)  acc5: 90.6250 (93.1775)  time: 15.0231  data: 0.0002  max mem: 50996
Test:  [125/131]  eta: 0:01:31  loss: 1.2667 (1.1638)  acc1: 72.6562 (76.7361)  acc5: 91.4062 (93.2147)  time: 15.0200  data: 0.0002  max mem: 50996
Test:  [130/131]  eta: 0:00:15  loss: 1.2163 (1.1710)  acc1: 73.9583 (76.7020)  acc5: 92.1875 (93.2560)  time: 14.4719  data: 0.0001  max mem: 50996
Test: Total time: 0:33:12 (15.2120 s / it)
* Acc@1 76.702 Acc@5 93.256 loss 1.171
Accuracy of the network on the 50000 test images: 76.7%
vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2 w8a8 method_1.5 -------------------------------------------------
Not using distributed mode
batch_size: 256
epochs: 300
bce_loss: False
unscale_lr: False
model: vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2
input_size: 224
drop: 0.0
drop_path: 0.1
model_ema: True
model_ema_decay: 0.99996
model_ema_force_cpu: False
opt: adamw
opt_eps: 1e-08
opt_betas: None
clip_grad: None
momentum: 0.9
weight_decay: 0.05
sched: cosine
lr: 0.0005
lr_noise: None
lr_noise_pct: 0.67
lr_noise_std: 1.0
warmup_lr: 1e-06
min_lr: 1e-05
decay_epochs: 30
warmup_epochs: 5
cooldown_epochs: 10
patience_epochs: 10
decay_rate: 0.1
color_jitter: 0.3
aa: rand-m9-mstd0.5-inc1
smoothing: 0.1
train_interpolation: bicubic
repeated_aug: True
train_mode: True
ThreeAugment: False
src: False
reprob: 0.25
remode: pixel
recount: 1
resplit: False
mixup: 0.8
cutmix: 1.0
cutmix_minmax: None
mixup_prob: 1.0
mixup_switch_prob: 0.5
mixup_mode: batch
teacher_model: regnety_160
teacher_path: 
distillation_type: none
distillation_alpha: 0.5
distillation_tau: 1.0
cosub: False
finetune: 
attn_only: False
data_path: /data01/datasets/imagenet
data_set: IMNET
inat_category: name
output_dir: 
device: cuda
seed: 0
resume: ./saved_checkpoint/vim_t+_midclstok_ft_78p3acc.pth
start_epoch: 0
eval: True
eval_crop_ratio: 0.875
dist_eval: False
num_workers: 10
pin_mem: True
distributed: False
world_size: 1
dist_url: env://
if_amp: True
if_continue_inf: False
if_nan2num: False
if_random_cls_token_position: False
if_random_token_rank: False
local_rank: 0
use_vim_torch: True
static_quant: True
observe: percentile
quant_weight: True
quant_act: True
a_bit: 8
w_bit: 8
use_smoothquant: False
use_gptq: False
use_hadmard: True
use_S1: False
use_S2: False
use_S3: False
use_S4: False
use_S5: False
use_S7: False
use_hadmard_R1: True
use_hadmard_R2: False
use_hadmard_R3: False
use_hadmard_R4: False
use_hadmard_R5: True
use_hadmard_R6: False
use_reduce_mean: True
use_pertoken: False
use_split: True
use_klt: True
generate_klt: False
use_perkernel: False
w_perchannel: True
fake_online_hadamard: False
analyse_and_plot: False
use_adaround: False
adaround_iter: 200
b_start: 20
b_end: 2
warmup: 0.2
Creating model: vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2
number of params: 7250344
Test:  [0/1]  eta: 0:01:23  loss: 0.7967 (0.7967)  acc1: 80.8000 (80.8000)  acc5: 96.8000 (96.8000)  time: 83.9060  data: 2.9691  max mem: 34328
Test: Total time: 0:01:24 (84.0078 s / it)
* Acc@1 80.800 Acc@5 96.800 loss 0.797
Fp Accuracy of the network on the 50000 test images: 80.8%
Test:  [  0/131]  eta: 1:43:00  loss: 2.3828 (2.3828)  acc1: 49.4792 (49.4792)  acc5: 82.8125 (82.8125)  time: 47.1775  data: 6.8420  max mem: 53624
Test:  [  5/131]  eta: 0:41:45  loss: 2.1744 (2.2354)  acc1: 61.7188 (59.2448)  acc5: 81.7708 (81.6840)  time: 19.8830  data: 1.1408  max mem: 53644
Test:  [ 10/131]  eta: 0:35:05  loss: 2.3774 (2.3596)  acc1: 52.0833 (54.8532)  acc5: 80.2083 (80.3977)  time: 17.4015  data: 0.6224  max mem: 53644
Test:  [ 15/131]  eta: 0:31:50  loss: 2.3774 (2.3757)  acc1: 52.0833 (55.9408)  acc5: 79.6875 (79.4271)  time: 16.4703  data: 0.4280  max mem: 53644
Test:  [ 20/131]  eta: 0:29:34  loss: 2.4127 (2.4035)  acc1: 52.0833 (56.0268)  acc5: 79.1667 (78.9187)  time: 14.4238  data: 0.0004  max mem: 53644
Test:  [ 25/131]  eta: 0:27:42  loss: 2.5582 (2.4787)  acc1: 51.3021 (53.6158)  acc5: 75.7812 (77.7444)  time: 14.4227  data: 0.0003  max mem: 53644
Test:  [ 30/131]  eta: 0:26:03  loss: 2.7154 (2.5423)  acc1: 46.6146 (52.1085)  acc5: 74.7396 (77.4194)  time: 14.4252  data: 0.0003  max mem: 53644
Test:  [ 35/131]  eta: 0:24:32  loss: 2.7154 (2.5675)  acc1: 46.3542 (51.2659)  acc5: 74.7396 (77.3727)  time: 14.4249  data: 0.0003  max mem: 53644
Test:  [ 40/131]  eta: 0:23:05  loss: 2.7775 (2.6258)  acc1: 46.0938 (50.1461)  acc5: 74.2188 (76.6451)  time: 14.4234  data: 0.0003  max mem: 53644
Test:  [ 45/131]  eta: 0:21:41  loss: 2.7775 (2.6107)  acc1: 46.3542 (50.6907)  acc5: 73.4375 (76.8003)  time: 14.4236  data: 0.0003  max mem: 53644
Test:  [ 50/131]  eta: 0:20:20  loss: 2.6271 (2.6063)  acc1: 46.6146 (50.4442)  acc5: 72.9167 (76.5625)  time: 14.4218  data: 0.0003  max mem: 53644
Test:  [ 55/131]  eta: 0:19:00  loss: 2.5479 (2.5861)  acc1: 49.2188 (50.8696)  acc5: 74.7396 (76.6881)  time: 14.4215  data: 0.0004  max mem: 53644
Test:  [ 60/131]  eta: 0:17:42  loss: 2.5479 (2.6251)  acc1: 49.4792 (50.2604)  acc5: 72.1354 (75.8837)  time: 14.4214  data: 0.0004  max mem: 53644
Test:  [ 65/131]  eta: 0:16:24  loss: 2.6582 (2.6664)  acc1: 45.3125 (49.2700)  acc5: 70.5729 (75.0986)  time: 14.4235  data: 0.0003  max mem: 53644
Test:  [ 70/131]  eta: 0:15:07  loss: 3.0437 (2.6902)  acc1: 44.0104 (48.7859)  acc5: 67.4479 (74.7066)  time: 14.4224  data: 0.0003  max mem: 53644
Test:  [ 75/131]  eta: 0:13:51  loss: 2.9586 (2.6726)  acc1: 44.0104 (49.1639)  acc5: 68.7500 (74.9623)  time: 14.4234  data: 0.0003  max mem: 53644
Test:  [ 80/131]  eta: 0:12:36  loss: 2.9586 (2.6993)  acc1: 43.2292 (48.7172)  acc5: 68.7500 (74.4406)  time: 14.4236  data: 0.0003  max mem: 53644
Test:  [ 85/131]  eta: 0:11:20  loss: 2.9737 (2.7226)  acc1: 44.0104 (48.1286)  acc5: 69.7917 (74.0916)  time: 14.4211  data: 0.0003  max mem: 53644
Test:  [ 90/131]  eta: 0:10:06  loss: 2.8176 (2.7289)  acc1: 46.6146 (48.0140)  acc5: 70.5729 (74.0156)  time: 14.4204  data: 0.0003  max mem: 53644
Test:  [ 95/131]  eta: 0:08:51  loss: 3.0380 (2.7415)  acc1: 43.4896 (47.7837)  acc5: 68.7500 (73.7169)  time: 14.4205  data: 0.0003  max mem: 53644
Test:  [100/131]  eta: 0:07:37  loss: 2.9737 (2.7472)  acc1: 44.7917 (47.7517)  acc5: 70.0521 (73.5200)  time: 14.4214  data: 0.0003  max mem: 53644
Test:  [105/131]  eta: 0:06:23  loss: 2.7157 (2.7451)  acc1: 46.0938 (47.8503)  acc5: 70.0521 (73.4154)  time: 14.4219  data: 0.0003  max mem: 53644
Test:  [110/131]  eta: 0:05:09  loss: 2.8342 (2.7492)  acc1: 46.0938 (47.7595)  acc5: 70.8333 (73.3272)  time: 14.4257  data: 0.0003  max mem: 53644
Test:  [115/131]  eta: 0:03:55  loss: 2.8434 (2.7589)  acc1: 46.0938 (47.6877)  acc5: 68.7500 (73.1120)  time: 14.4249  data: 0.0002  max mem: 53644
Test:  [120/131]  eta: 0:02:41  loss: 2.8434 (2.7573)  acc1: 46.0938 (47.7014)  acc5: 70.8333 (73.1448)  time: 14.4240  data: 0.0002  max mem: 53644
Test:  [125/131]  eta: 0:01:28  loss: 2.8434 (2.7695)  acc1: 46.0938 (47.5715)  acc5: 70.8333 (72.9642)  time: 14.4234  data: 0.0002  max mem: 53644
Test:  [130/131]  eta: 0:00:14  loss: 2.8877 (2.7648)  acc1: 45.5729 (47.7300)  acc5: 70.3125 (73.0800)  time: 13.8814  data: 0.0001  max mem: 53644
Test: Total time: 0:31:51 (14.5922 s / it)
* Acc@1 47.730 Acc@5 73.080 loss 2.765
Accuracy of the network on the 50000 test images: 47.7%
vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2 w8a8 method_0.5 -------------------------------------------------
Not using distributed mode
batch_size: 128
epochs: 300
bce_loss: False
unscale_lr: False
model: deit_base_patch16_224
input_size: 224
drop: 0.0
drop_path: 0.1
model_ema: True
model_ema_decay: 0.99996
model_ema_force_cpu: False
opt: adamw
opt_eps: 1e-08
opt_betas: None
clip_grad: None
momentum: 0.9
weight_decay: 0.05
sched: cosine
lr: 0.0005
lr_noise: None
lr_noise_pct: 0.67
lr_noise_std: 1.0
warmup_lr: 1e-06
min_lr: 1e-05
decay_epochs: 30
warmup_epochs: 5
cooldown_epochs: 10
patience_epochs: 10
decay_rate: 0.1
color_jitter: 0.3
aa: rand-m9-mstd0.5-inc1
smoothing: 0.1
train_interpolation: bicubic
repeated_aug: True
train_mode: True
ThreeAugment: False
src: False
reprob: 0.25
remode: pixel
recount: 1
resplit: False
mixup: 0.8
cutmix: 1.0
cutmix_minmax: None
mixup_prob: 1.0
mixup_switch_prob: 0.5
mixup_mode: batch
teacher_model: regnety_160
teacher_path: 
distillation_type: none
distillation_alpha: 0.5
distillation_tau: 1.0
cosub: False
finetune: 
attn_only: False
data_path: /datasets01/imagenet_full_size/061417/
data_set: IMNET
inat_category: name
output_dir: 
device: cuda
seed: 0
resume: 
start_epoch: 0
eval: False
eval_crop_ratio: 0.875
dist_eval: False
num_workers: 10
pin_mem: True
distributed: False
world_size: 1
dist_url: env://
if_amp: True
if_continue_inf: False
if_nan2num: False
if_random_cls_token_position: False
if_random_token_rank: False
local_rank: 0
use_vim_torch: False
static_quant: False
observe: minmax
quant_weight: False
quant_act: False
a_bit: 8
w_bit: 8
use_smoothquant: False
use_gptq: False
use_hadmard: False
use_S1: False
use_S2: False
use_S3: False
use_S4: False
use_S5: False
use_S7: False
use_hadmard_R1: False
use_hadmard_R2: False
use_hadmard_R3: False
use_hadmard_R4: False
use_hadmard_R5: False
use_hadmard_R6: False
use_reduce_mean: False
use_pertoken: False
use_split: False
use_klt: False
generate_klt: False
use_perkernel: False
w_perchannel: False
fake_online_hadamard: False
analyse_and_plot: False
use_adaround: False
adaround_iter: 200
b_start: 20
b_end: 2
warmup: 0.2
vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2 w4a8 method_2 -------------------------------------------------
Not using distributed mode
batch_size: 256
epochs: 300
bce_loss: False
unscale_lr: False
model: vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2
input_size: 224
drop: 0.0
drop_path: 0.1
model_ema: True
model_ema_decay: 0.99996
model_ema_force_cpu: False
opt: adamw
opt_eps: 1e-08
opt_betas: None
clip_grad: None
momentum: 0.9
weight_decay: 0.05
sched: cosine
lr: 0.0005
lr_noise: None
lr_noise_pct: 0.67
lr_noise_std: 1.0
warmup_lr: 1e-06
min_lr: 1e-05
decay_epochs: 30
warmup_epochs: 5
cooldown_epochs: 10
patience_epochs: 10
decay_rate: 0.1
color_jitter: 0.3
aa: rand-m9-mstd0.5-inc1
smoothing: 0.1
train_interpolation: bicubic
repeated_aug: True
train_mode: True
ThreeAugment: False
src: False
reprob: 0.25
remode: pixel
recount: 1
resplit: False
mixup: 0.8
cutmix: 1.0
cutmix_minmax: None
mixup_prob: 1.0
mixup_switch_prob: 0.5
mixup_mode: batch
teacher_model: regnety_160
teacher_path: 
distillation_type: none
distillation_alpha: 0.5
distillation_tau: 1.0
cosub: False
finetune: 
attn_only: False
data_path: /data01/datasets/imagenet
data_set: IMNET
inat_category: name
output_dir: 
device: cuda
seed: 0
resume: ./saved_checkpoint/vim_t+_midclstok_ft_78p3acc.pth
start_epoch: 0
eval: True
eval_crop_ratio: 0.875
dist_eval: False
num_workers: 10
pin_mem: True
distributed: False
world_size: 1
dist_url: env://
if_amp: True
if_continue_inf: False
if_nan2num: False
if_random_cls_token_position: False
if_random_token_rank: False
local_rank: 0
use_vim_torch: True
static_quant: True
observe: percentile
quant_weight: True
quant_act: True
a_bit: 8
w_bit: 4
use_smoothquant: False
use_gptq: False
use_hadmard: True
use_S1: False
use_S2: True
use_S3: True
use_S4: False
use_S5: False
use_S7: False
use_hadmard_R1: True
use_hadmard_R2: True
use_hadmard_R3: False
use_hadmard_R4: True
use_hadmard_R5: True
use_hadmard_R6: False
use_reduce_mean: True
use_pertoken: True
use_split: True
use_klt: True
generate_klt: False
use_perkernel: False
w_perchannel: True
fake_online_hadamard: False
analyse_and_plot: False
use_adaround: False
adaround_iter: 200
b_start: 20
b_end: 2
warmup: 0.2
Creating model: vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2
number of params: 7250344
Test:  [0/1]  eta: 0:00:42  loss: 0.7967 (0.7967)  acc1: 80.8000 (80.8000)  acc5: 96.8000 (96.8000)  time: 42.8146  data: 2.4325  max mem: 30495
Test: Total time: 0:00:42 (42.9058 s / it)
* Acc@1 80.800 Acc@5 96.800 loss 0.797
Fp Accuracy of the network on the 50000 test images: 80.8%
Test:  [  0/131]  eta: 1:58:51  loss: 1.0913 (1.0913)  acc1: 80.2083 (80.2083)  acc5: 96.3542 (96.3542)  time: 54.4394  data: 7.0316  max mem: 47733
Test:  [  5/131]  eta: 0:46:53  loss: 1.0330 (1.0538)  acc1: 80.9896 (83.2031)  acc5: 95.5729 (96.1372)  time: 22.3274  data: 1.1723  max mem: 47754
Test:  [ 10/131]  eta: 0:39:06  loss: 1.1265 (1.1826)  acc1: 80.2083 (79.2614)  acc5: 95.5729 (95.4782)  time: 19.3951  data: 0.6396  max mem: 47754
Test:  [ 15/131]  eta: 0:35:22  loss: 1.1000 (1.1522)  acc1: 80.4688 (81.2663)  acc5: 95.3125 (95.3125)  time: 18.2937  data: 0.4398  max mem: 47754
Test:  [ 20/131]  eta: 0:32:46  loss: 1.0764 (1.1000)  acc1: 83.3333 (82.7257)  acc5: 95.5729 (95.6721)  time: 15.8812  data: 0.0003  max mem: 47754
Test:  [ 25/131]  eta: 0:30:40  loss: 1.1265 (1.1638)  acc1: 79.4271 (80.7091)  acc5: 95.0521 (95.1823)  time: 15.8725  data: 0.0003  max mem: 47754
Test:  [ 30/131]  eta: 0:28:49  loss: 1.1767 (1.1727)  acc1: 80.4688 (80.3763)  acc5: 94.5312 (95.0857)  time: 15.8705  data: 0.0003  max mem: 47754
Test:  [ 35/131]  eta: 0:27:06  loss: 1.1790 (1.1664)  acc1: 77.8646 (80.2300)  acc5: 95.5729 (95.2763)  time: 15.8698  data: 0.0003  max mem: 47754
Test:  [ 40/131]  eta: 0:25:30  loss: 1.2204 (1.1745)  acc1: 76.8229 (80.0051)  acc5: 95.3125 (95.2871)  time: 15.8686  data: 0.0003  max mem: 47754
Test:  [ 45/131]  eta: 0:23:57  loss: 1.1556 (1.1570)  acc1: 81.2500 (80.6159)  acc5: 96.0938 (95.4427)  time: 15.8694  data: 0.0003  max mem: 47754
Test:  [ 50/131]  eta: 0:22:27  loss: 1.1343 (1.1578)  acc1: 81.2500 (80.5249)  acc5: 96.0938 (95.4197)  time: 15.8694  data: 0.0003  max mem: 47754
Test:  [ 55/131]  eta: 0:20:58  loss: 1.1343 (1.1869)  acc1: 79.9479 (79.8270)  acc5: 94.7917 (95.0381)  time: 15.8697  data: 0.0003  max mem: 47754
Test:  [ 60/131]  eta: 0:19:31  loss: 1.2531 (1.2375)  acc1: 75.2604 (78.6800)  acc5: 92.7083 (94.3818)  time: 15.8697  data: 0.0003  max mem: 47754
Test:  [ 65/131]  eta: 0:18:06  loss: 1.5976 (1.2922)  acc1: 68.4896 (77.4740)  acc5: 89.0625 (93.5961)  time: 15.8682  data: 0.0003  max mem: 47754
Test:  [ 70/131]  eta: 0:16:41  loss: 1.6842 (1.3150)  acc1: 68.2292 (76.9036)  acc5: 88.2812 (93.3429)  time: 15.8683  data: 0.0003  max mem: 47754
Test:  [ 75/131]  eta: 0:15:17  loss: 1.6842 (1.3188)  acc1: 68.4896 (76.9668)  acc5: 88.2812 (93.2463)  time: 15.8684  data: 0.0003  max mem: 47754
Test:  [ 80/131]  eta: 0:13:53  loss: 1.6534 (1.3430)  acc1: 69.7917 (76.4564)  acc5: 89.3229 (92.9141)  time: 15.8697  data: 0.0003  max mem: 47754
Test:  [ 85/131]  eta: 0:12:30  loss: 1.6684 (1.3752)  acc1: 69.7917 (75.7873)  acc5: 89.8438 (92.5115)  time: 15.8707  data: 0.0003  max mem: 47754
Test:  [ 90/131]  eta: 0:11:08  loss: 1.7171 (1.3909)  acc1: 70.0521 (75.4264)  acc5: 87.5000 (92.3249)  time: 15.8706  data: 0.0003  max mem: 47754
Test:  [ 95/131]  eta: 0:09:45  loss: 1.7246 (1.4072)  acc1: 67.9688 (75.2143)  acc5: 87.5000 (92.1414)  time: 15.8709  data: 0.0003  max mem: 47754
Test:  [100/131]  eta: 0:08:23  loss: 1.7246 (1.4205)  acc1: 67.9688 (74.8917)  acc5: 87.5000 (91.9322)  time: 15.8709  data: 0.0003  max mem: 47754
Test:  [105/131]  eta: 0:07:02  loss: 1.6779 (1.4342)  acc1: 69.7917 (74.5897)  acc5: 87.5000 (91.7133)  time: 15.8732  data: 0.0003  max mem: 47754
Test:  [110/131]  eta: 0:05:40  loss: 1.7047 (1.4539)  acc1: 68.7500 (74.1343)  acc5: 87.5000 (91.4344)  time: 15.8741  data: 0.0003  max mem: 47754
Test:  [115/131]  eta: 0:04:19  loss: 1.6778 (1.4622)  acc1: 68.4896 (73.9471)  acc5: 88.0208 (91.3232)  time: 15.8744  data: 0.0003  max mem: 47754
Test:  [120/131]  eta: 0:02:58  loss: 1.7047 (1.4749)  acc1: 67.9688 (73.6269)  acc5: 88.2812 (91.2298)  time: 16.0343  data: 0.0002  max mem: 47754
Test:  [125/131]  eta: 0:01:37  loss: 1.6746 (1.4750)  acc1: 67.9688 (73.6917)  acc5: 88.8021 (91.2678)  time: 16.2139  data: 0.0002  max mem: 47754
Test:  [130/131]  eta: 0:00:16  loss: 1.6462 (1.4811)  acc1: 67.9688 (73.6680)  acc5: 89.5833 (91.2980)  time: 15.7265  data: 0.0002  max mem: 47754
Test: Total time: 0:35:15 (16.1459 s / it)
* Acc@1 73.668 Acc@5 91.298 loss 1.481
Accuracy of the network on the 50000 test images: 73.7%
vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2 w4a8 method_1.5 -------------------------------------------------
Not using distributed mode
batch_size: 256
epochs: 300
bce_loss: False
unscale_lr: False
model: vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2
input_size: 224
drop: 0.0
drop_path: 0.1
model_ema: True
model_ema_decay: 0.99996
model_ema_force_cpu: False
opt: adamw
opt_eps: 1e-08
opt_betas: None
clip_grad: None
momentum: 0.9
weight_decay: 0.05
sched: cosine
lr: 0.0005
lr_noise: None
lr_noise_pct: 0.67
lr_noise_std: 1.0
warmup_lr: 1e-06
min_lr: 1e-05
decay_epochs: 30
warmup_epochs: 5
cooldown_epochs: 10
patience_epochs: 10
decay_rate: 0.1
color_jitter: 0.3
aa: rand-m9-mstd0.5-inc1
smoothing: 0.1
train_interpolation: bicubic
repeated_aug: True
train_mode: True
ThreeAugment: False
src: False
reprob: 0.25
remode: pixel
recount: 1
resplit: False
mixup: 0.8
cutmix: 1.0
cutmix_minmax: None
mixup_prob: 1.0
mixup_switch_prob: 0.5
mixup_mode: batch
teacher_model: regnety_160
teacher_path: 
distillation_type: none
distillation_alpha: 0.5
distillation_tau: 1.0
cosub: False
finetune: 
attn_only: False
data_path: /data01/datasets/imagenet
data_set: IMNET
inat_category: name
output_dir: 
device: cuda
seed: 0
resume: ./saved_checkpoint/vim_t+_midclstok_ft_78p3acc.pth
start_epoch: 0
eval: True
eval_crop_ratio: 0.875
dist_eval: False
num_workers: 10
pin_mem: True
distributed: False
world_size: 1
dist_url: env://
if_amp: True
if_continue_inf: False
if_nan2num: False
if_random_cls_token_position: False
if_random_token_rank: False
local_rank: 0
use_vim_torch: True
static_quant: True
observe: percentile
quant_weight: True
quant_act: True
a_bit: 8
w_bit: 4
use_smoothquant: False
use_gptq: False
use_hadmard: True
use_S1: False
use_S2: True
use_S3: True
use_S4: False
use_S5: False
use_S7: False
use_hadmard_R1: True
use_hadmard_R2: False
use_hadmard_R3: False
use_hadmard_R4: False
use_hadmard_R5: True
use_hadmard_R6: False
use_reduce_mean: True
use_pertoken: True
use_split: True
use_klt: True
generate_klt: False
use_perkernel: False
w_perchannel: True
fake_online_hadamard: False
analyse_and_plot: False
use_adaround: False
adaround_iter: 200
b_start: 20
b_end: 2
warmup: 0.2
Creating model: vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2
number of params: 7250344
Test:  [0/1]  eta: 0:00:21  loss: 0.7966 (0.7966)  acc1: 80.8000 (80.8000)  acc5: 96.8000 (96.8000)  time: 21.1144  data: 2.3779  max mem: 30493
Test: Total time: 0:00:21 (21.2078 s / it)
* Acc@1 80.800 Acc@5 96.800 loss 0.797
Fp Accuracy of the network on the 50000 test images: 80.8%
Test:  [  0/131]  eta: 0:59:52  loss: 1.4729 (1.4729)  acc1: 78.9062 (78.9062)  acc5: 93.7500 (93.7500)  time: 27.4199  data: 5.1987  max mem: 50976
Test:  [  5/131]  eta: 0:36:55  loss: 1.4691 (1.5668)  acc1: 80.7292 (81.5538)  acc5: 93.7500 (94.0538)  time: 17.5864  data: 0.8667  max mem: 50996
Test:  [ 10/131]  eta: 0:33:07  loss: 1.6058 (1.6650)  acc1: 80.4688 (77.2964)  acc5: 93.7500 (93.0398)  time: 16.4290  data: 0.4729  max mem: 50996
Test:  [ 15/131]  eta: 0:30:54  loss: 1.5508 (1.6111)  acc1: 80.7292 (79.3620)  acc5: 93.7500 (93.6523)  time: 15.9903  data: 0.3252  max mem: 50996
Test:  [ 20/131]  eta: 0:29:09  loss: 1.5245 (1.5519)  acc1: 80.9896 (80.4688)  acc5: 94.7917 (94.3080)  time: 15.1786  data: 0.0003  max mem: 50996
Test:  [ 25/131]  eta: 0:27:35  loss: 1.5994 (1.6295)  acc1: 77.0833 (78.4756)  acc5: 94.7917 (93.8502)  time: 15.0301  data: 0.0003  max mem: 50996
Test:  [ 30/131]  eta: 0:26:07  loss: 1.6894 (1.6971)  acc1: 76.0417 (77.3354)  acc5: 93.4896 (93.5148)  time: 15.0266  data: 0.0003  max mem: 50996
Test:  [ 35/131]  eta: 0:24:43  loss: 1.8845 (1.7217)  acc1: 72.3958 (76.8591)  acc5: 93.4896 (93.6632)  time: 15.0265  data: 0.0003  max mem: 50996
Test:  [ 40/131]  eta: 0:23:21  loss: 1.8874 (1.7325)  acc1: 72.1354 (76.7149)  acc5: 92.9688 (93.7119)  time: 15.0262  data: 0.0003  max mem: 50996
Test:  [ 45/131]  eta: 0:22:01  loss: 1.7350 (1.6925)  acc1: 75.7812 (77.3947)  acc5: 93.4896 (93.9821)  time: 15.0264  data: 0.0003  max mem: 50996
Test:  [ 50/131]  eta: 0:20:41  loss: 1.6558 (1.6749)  acc1: 78.6458 (77.4561)  acc5: 94.0104 (94.0155)  time: 15.0270  data: 0.0004  max mem: 50996
Test:  [ 55/131]  eta: 0:19:22  loss: 1.6807 (1.6969)  acc1: 78.6458 (76.9113)  acc5: 93.2292 (93.5686)  time: 15.0283  data: 0.0004  max mem: 50996
Test:  [ 60/131]  eta: 0:18:04  loss: 1.6931 (1.7428)  acc1: 72.1354 (75.7684)  acc5: 93.2292 (92.9261)  time: 15.0297  data: 0.0004  max mem: 50996
Test:  [ 65/131]  eta: 0:16:47  loss: 2.1215 (1.8029)  acc1: 66.4062 (74.4752)  acc5: 86.4583 (92.0139)  time: 15.0300  data: 0.0003  max mem: 50996
Test:  [ 70/131]  eta: 0:15:29  loss: 2.2318 (1.8324)  acc1: 63.0208 (73.6979)  acc5: 85.9375 (91.6593)  time: 15.0298  data: 0.0003  max mem: 50996
Test:  [ 75/131]  eta: 0:14:12  loss: 2.2301 (1.8378)  acc1: 63.8021 (73.7013)  acc5: 85.9375 (91.5296)  time: 15.0309  data: 0.0003  max mem: 50996
Test:  [ 80/131]  eta: 0:12:56  loss: 2.2318 (1.8714)  acc1: 62.5000 (72.9488)  acc5: 85.9375 (91.0205)  time: 15.0291  data: 0.0003  max mem: 50996
Test:  [ 85/131]  eta: 0:11:39  loss: 2.2301 (1.9105)  acc1: 63.0208 (72.1445)  acc5: 86.4583 (90.5069)  time: 15.0298  data: 0.0003  max mem: 50996
Test:  [ 90/131]  eta: 0:10:23  loss: 2.1963 (1.9288)  acc1: 62.5000 (71.6546)  acc5: 86.4583 (90.2902)  time: 15.0296  data: 0.0003  max mem: 50996
Test:  [ 95/131]  eta: 0:09:06  loss: 2.3394 (1.9452)  acc1: 62.5000 (71.3189)  acc5: 83.8542 (90.0364)  time: 15.0275  data: 0.0003  max mem: 50996
Test:  [100/131]  eta: 0:07:50  loss: 2.3197 (1.9596)  acc1: 63.0208 (71.0216)  acc5: 85.1562 (89.8489)  time: 15.0288  data: 0.0003  max mem: 50996
Test:  [105/131]  eta: 0:06:34  loss: 2.1752 (1.9698)  acc1: 65.1042 (70.7645)  acc5: 85.9375 (89.6374)  time: 15.0297  data: 0.0003  max mem: 50996
Test:  [110/131]  eta: 0:05:18  loss: 2.1752 (1.9847)  acc1: 65.1042 (70.4415)  acc5: 85.1562 (89.4520)  time: 15.0320  data: 0.0003  max mem: 50996
Test:  [115/131]  eta: 0:04:02  loss: 2.2510 (2.0012)  acc1: 64.8438 (70.1980)  acc5: 84.8958 (89.2354)  time: 15.0319  data: 0.0003  max mem: 50996
Test:  [120/131]  eta: 0:02:46  loss: 2.2660 (2.0123)  acc1: 64.3229 (69.8240)  acc5: 84.6354 (89.0819)  time: 15.0307  data: 0.0002  max mem: 50996
Test:  [125/131]  eta: 0:01:30  loss: 2.2510 (2.0060)  acc1: 66.4062 (69.9839)  acc5: 84.8958 (89.1100)  time: 15.0290  data: 0.0002  max mem: 50996
Test:  [130/131]  eta: 0:00:15  loss: 2.1701 (2.0061)  acc1: 66.4062 (70.0480)  acc5: 86.4583 (89.2260)  time: 14.4584  data: 0.0001  max mem: 50996
Test: Total time: 0:32:52 (15.0608 s / it)
* Acc@1 70.048 Acc@5 89.226 loss 2.006
Accuracy of the network on the 50000 test images: 70.0%
vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2 w4a8 method_1.5 -------------------------------------------------
Not using distributed mode
batch_size: 256
epochs: 300
bce_loss: False
unscale_lr: False
model: vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2
input_size: 224
drop: 0.0
drop_path: 0.1
model_ema: True
model_ema_decay: 0.99996
model_ema_force_cpu: False
opt: adamw
opt_eps: 1e-08
opt_betas: None
clip_grad: None
momentum: 0.9
weight_decay: 0.05
sched: cosine
lr: 0.0005
lr_noise: None
lr_noise_pct: 0.67
lr_noise_std: 1.0
warmup_lr: 1e-06
min_lr: 1e-05
decay_epochs: 30
warmup_epochs: 5
cooldown_epochs: 10
patience_epochs: 10
decay_rate: 0.1
color_jitter: 0.3
aa: rand-m9-mstd0.5-inc1
smoothing: 0.1
train_interpolation: bicubic
repeated_aug: True
train_mode: True
ThreeAugment: False
src: False
reprob: 0.25
remode: pixel
recount: 1
resplit: False
mixup: 0.8
cutmix: 1.0
cutmix_minmax: None
mixup_prob: 1.0
mixup_switch_prob: 0.5
mixup_mode: batch
teacher_model: regnety_160
teacher_path: 
distillation_type: none
distillation_alpha: 0.5
distillation_tau: 1.0
cosub: False
finetune: 
attn_only: False
data_path: /data01/datasets/imagenet
data_set: IMNET
inat_category: name
output_dir: 
device: cuda
seed: 0
resume: ./saved_checkpoint/vim_t+_midclstok_ft_78p3acc.pth
start_epoch: 0
eval: True
eval_crop_ratio: 0.875
dist_eval: False
num_workers: 10
pin_mem: True
distributed: False
world_size: 1
dist_url: env://
if_amp: True
if_continue_inf: False
if_nan2num: False
if_random_cls_token_position: False
if_random_token_rank: False
local_rank: 0
use_vim_torch: True
static_quant: True
observe: percentile
quant_weight: True
quant_act: True
a_bit: 8
w_bit: 4
use_smoothquant: False
use_gptq: False
use_hadmard: True
use_S1: False
use_S2: False
use_S3: False
use_S4: False
use_S5: False
use_S7: False
use_hadmard_R1: True
use_hadmard_R2: False
use_hadmard_R3: False
use_hadmard_R4: False
use_hadmard_R5: True
use_hadmard_R6: False
use_reduce_mean: True
use_pertoken: False
use_split: True
use_klt: True
generate_klt: False
use_perkernel: False
w_perchannel: True
fake_online_hadamard: False
analyse_and_plot: False
use_adaround: False
adaround_iter: 200
b_start: 20
b_end: 2
warmup: 0.2
Creating model: vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2
number of params: 7250344
Test:  [0/1]  eta: 0:01:05  loss: 0.7967 (0.7967)  acc1: 80.8000 (80.8000)  acc5: 96.8000 (96.8000)  time: 65.2952  data: 2.3891  max mem: 34328
Test: Total time: 0:01:05 (65.3947 s / it)
* Acc@1 80.800 Acc@5 96.800 loss 0.797
Fp Accuracy of the network on the 50000 test images: 80.8%
Test:  [  0/131]  eta: 1:09:21  loss: 3.2454 (3.2454)  acc1: 38.5417 (38.5417)  acc5: 67.7083 (67.7083)  time: 31.7649  data: 5.8453  max mem: 53624
Test:  [  5/131]  eta: 0:36:21  loss: 2.7201 (2.8158)  acc1: 51.3021 (50.8247)  acc5: 72.9167 (72.8299)  time: 17.3157  data: 0.9745  max mem: 53644
Test:  [ 10/131]  eta: 0:32:16  loss: 2.8028 (2.9116)  acc1: 46.6146 (47.0407)  acc5: 71.8750 (71.8513)  time: 16.0019  data: 0.5317  max mem: 53644
Test:  [ 15/131]  eta: 0:29:59  loss: 2.8028 (2.9653)  acc1: 46.3542 (47.3796)  acc5: 71.8750 (70.9635)  time: 15.5113  data: 0.3656  max mem: 53644
Test:  [ 20/131]  eta: 0:28:13  loss: 3.0011 (3.0168)  acc1: 45.3125 (46.6518)  acc5: 67.7083 (69.9777)  time: 14.4271  data: 0.0003  max mem: 53644
Test:  [ 25/131]  eta: 0:26:39  loss: 3.3244 (3.1186)  acc1: 39.5833 (43.8902)  acc5: 65.8854 (68.3794)  time: 14.4265  data: 0.0003  max mem: 53644
Test:  [ 30/131]  eta: 0:25:13  loss: 3.3927 (3.2075)  acc1: 37.7604 (41.8683)  acc5: 63.0208 (67.3555)  time: 14.4256  data: 0.0003  max mem: 53644
Test:  [ 35/131]  eta: 0:23:51  loss: 3.3944 (3.2490)  acc1: 34.1146 (40.5816)  acc5: 63.0208 (66.8982)  time: 14.4230  data: 0.0003  max mem: 53644
Test:  [ 40/131]  eta: 0:22:31  loss: 3.5383 (3.3059)  acc1: 30.7292 (39.5833)  acc5: 61.4583 (66.1903)  time: 14.4219  data: 0.0003  max mem: 53644
Test:  [ 45/131]  eta: 0:21:12  loss: 3.5257 (3.2814)  acc1: 35.1562 (40.4495)  acc5: 61.4583 (66.6553)  time: 14.4213  data: 0.0004  max mem: 53644
Test:  [ 50/131]  eta: 0:19:55  loss: 3.2743 (3.2715)  acc1: 35.6771 (40.1348)  acc5: 61.4583 (66.3552)  time: 14.4212  data: 0.0004  max mem: 53644
Test:  [ 55/131]  eta: 0:18:39  loss: 3.2251 (3.2567)  acc1: 36.9792 (40.3739)  acc5: 63.0208 (66.5225)  time: 14.4216  data: 0.0004  max mem: 53644
Test:  [ 60/131]  eta: 0:17:24  loss: 3.2251 (3.2909)  acc1: 38.8021 (39.8907)  acc5: 63.0208 (65.8128)  time: 14.4225  data: 0.0003  max mem: 53644
Test:  [ 65/131]  eta: 0:16:09  loss: 3.4339 (3.3275)  acc1: 35.6771 (39.2085)  acc5: 59.8958 (65.0410)  time: 14.4234  data: 0.0003  max mem: 53644
Test:  [ 70/131]  eta: 0:14:54  loss: 3.6361 (3.3453)  acc1: 34.3750 (38.8461)  acc5: 58.8542 (64.7007)  time: 14.4257  data: 0.0003  max mem: 53644
Test:  [ 75/131]  eta: 0:13:40  loss: 3.5072 (3.3250)  acc1: 34.6354 (39.3812)  acc5: 59.8958 (64.9979)  time: 14.4248  data: 0.0003  max mem: 53644
Test:  [ 80/131]  eta: 0:12:26  loss: 3.5072 (3.3452)  acc1: 35.1562 (39.1590)  acc5: 59.8958 (64.5351)  time: 14.4262  data: 0.0003  max mem: 53644
Test:  [ 85/131]  eta: 0:11:12  loss: 3.4860 (3.3593)  acc1: 35.1562 (38.7839)  acc5: 62.7604 (64.3048)  time: 14.4285  data: 0.0003  max mem: 53644
Test:  [ 90/131]  eta: 0:09:59  loss: 3.3254 (3.3637)  acc1: 38.5417 (38.7305)  acc5: 63.0208 (64.3744)  time: 14.4271  data: 0.0003  max mem: 53644
Test:  [ 95/131]  eta: 0:08:45  loss: 3.5112 (3.3737)  acc1: 35.4167 (38.5200)  acc5: 62.2396 (64.0815)  time: 14.4280  data: 0.0003  max mem: 53644
Test:  [100/131]  eta: 0:07:32  loss: 3.3735 (3.3727)  acc1: 38.0208 (38.5726)  acc5: 62.5000 (63.9903)  time: 14.4265  data: 0.0003  max mem: 53644
Test:  [105/131]  eta: 0:06:19  loss: 3.3545 (3.3662)  acc1: 38.5417 (38.7923)  acc5: 62.5000 (64.0134)  time: 14.4261  data: 0.0003  max mem: 53644
Test:  [110/131]  eta: 0:05:06  loss: 3.3735 (3.3690)  acc1: 38.8021 (38.8044)  acc5: 61.4583 (63.9124)  time: 14.4256  data: 0.0003  max mem: 53644
Test:  [115/131]  eta: 0:03:53  loss: 3.4609 (3.3767)  acc1: 39.3229 (38.8335)  acc5: 60.1562 (63.7931)  time: 14.4252  data: 0.0002  max mem: 53644
Test:  [120/131]  eta: 0:02:40  loss: 3.4609 (3.3729)  acc1: 38.5417 (38.8925)  acc5: 61.7188 (63.8473)  time: 14.4249  data: 0.0002  max mem: 53644
Test:  [125/131]  eta: 0:01:27  loss: 3.5326 (3.3912)  acc1: 38.5417 (38.7587)  acc5: 61.7188 (63.5706)  time: 14.4226  data: 0.0002  max mem: 53644
Test:  [130/131]  eta: 0:00:14  loss: 3.5326 (3.3843)  acc1: 36.4583 (38.8520)  acc5: 63.2812 (63.7200)  time: 13.8570  data: 0.0001  max mem: 53644
Test: Total time: 0:31:35 (14.4725 s / it)
* Acc@1 38.852 Acc@5 63.720 loss 3.384
Accuracy of the network on the 50000 test images: 38.9%
vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2 w4a8 method_0.5 -------------------------------------------------
Not using distributed mode
batch_size: 256
epochs: 300
bce_loss: False
unscale_lr: False
model: vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2
input_size: 224
drop: 0.0
drop_path: 0.1
model_ema: True
model_ema_decay: 0.99996
model_ema_force_cpu: False
opt: adamw
opt_eps: 1e-08
opt_betas: None
clip_grad: None
momentum: 0.9
weight_decay: 0.05
sched: cosine
lr: 0.0005
lr_noise: None
lr_noise_pct: 0.67
lr_noise_std: 1.0
warmup_lr: 1e-06
min_lr: 1e-05
decay_epochs: 30
warmup_epochs: 5
cooldown_epochs: 10
patience_epochs: 10
decay_rate: 0.1
color_jitter: 0.3
aa: rand-m9-mstd0.5-inc1
smoothing: 0.1
train_interpolation: bicubic
repeated_aug: True
train_mode: True
ThreeAugment: False
src: False
reprob: 0.25
remode: pixel
recount: 1
resplit: False
mixup: 0.8
cutmix: 1.0
cutmix_minmax: None
mixup_prob: 1.0
mixup_switch_prob: 0.5
mixup_mode: batch
teacher_model: regnety_160
teacher_path: 
distillation_type: none
distillation_alpha: 0.5
distillation_tau: 1.0
cosub: False
finetune: 
attn_only: False
data_path: /data01/datasets/imagenet
data_set: IMNET
inat_category: name
output_dir: 
device: cuda
seed: 0
resume: ./saved_checkpoint/vim_t+_midclstok_ft_78p3acc.pth
start_epoch: 0
eval: True
eval_crop_ratio: 0.875
dist_eval: False
num_workers: 10
pin_mem: True
distributed: False
world_size: 1
dist_url: env://
if_amp: True
if_continue_inf: False
if_nan2num: False
if_random_cls_token_position: False
if_random_token_rank: False
local_rank: 0
use_vim_torch: True
static_quant: True
observe: minmax
quant_weight: True
quant_act: True
a_bit: 8
w_bit: 4
use_smoothquant: False
use_gptq: False
use_hadmard: True
use_S1: False
use_S2: False
use_S3: False
use_S4: False
use_S5: False
use_S7: False
use_hadmard_R1: True
use_hadmard_R2: False
use_hadmard_R3: False
use_hadmard_R4: False
use_hadmard_R5: True
use_hadmard_R6: False
use_reduce_mean: False
use_pertoken: False
use_split: False
use_klt: False
generate_klt: False
use_perkernel: False
w_perchannel: True
fake_online_hadamard: False
analyse_and_plot: False
use_adaround: False
adaround_iter: 200
b_start: 20
b_end: 2
warmup: 0.2
Creating model: vim_tinyplus_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2
number of params: 7250344
Test:  [0/1]  eta: 0:00:11  loss: 0.7965 (0.7965)  acc1: 80.8000 (80.8000)  acc5: 96.8000 (96.8000)  time: 11.6273  data: 2.4395  max mem: 30416
Test: Total time: 0:00:11 (11.7288 s / it)
* Acc@1 80.800 Acc@5 96.800 loss 0.797
Fp Accuracy of the network on the 50000 test images: 80.8%
Test:  [  0/131]  eta: 0:48:41  loss: 4.6960 (4.6960)  acc1: 21.6146 (21.6146)  acc5: 46.8750 (46.8750)  time: 22.3031  data: 5.2646  max mem: 52565
Test:  [  5/131]  eta: 0:33:00  loss: 4.6273 (4.5438)  acc1: 24.4792 (28.0816)  acc5: 45.3125 (46.3976)  time: 15.7196  data: 0.8777  max mem: 52567
Test:  [ 10/131]  eta: 0:30:29  loss: 4.6960 (4.6063)  acc1: 25.7812 (26.1364)  acc5: 46.8750 (45.7860)  time: 15.1182  data: 0.4789  max mem: 52567
Test:  [ 15/131]  eta: 0:28:47  loss: 4.6273 (4.6192)  acc1: 25.7812 (26.3509)  acc5: 45.3125 (44.5964)  time: 14.8928  data: 0.3293  max mem: 52567
Test:  [ 20/131]  eta: 0:27:19  loss: 4.6742 (4.6905)  acc1: 24.4792 (25.2976)  acc5: 44.2708 (42.8323)  time: 14.3983  data: 0.0003  max mem: 52567
Test:  [ 25/131]  eta: 0:25:58  loss: 4.7567 (4.7758)  acc1: 18.7500 (22.9067)  acc5: 38.5417 (40.5649)  time: 14.3984  data: 0.0003  max mem: 52567
Test:  [ 30/131]  eta: 0:24:40  loss: 5.0766 (4.8667)  acc1: 17.4479 (21.3542)  acc5: 35.6771 (39.0037)  time: 14.3991  data: 0.0003  max mem: 52567
Test:  [ 35/131]  eta: 0:23:23  loss: 5.1321 (4.8935)  acc1: 14.8438 (20.3776)  acc5: 33.5938 (38.6212)  time: 14.3998  data: 0.0003  max mem: 52567
Test:  [ 40/131]  eta: 0:22:07  loss: 5.1720 (4.9828)  acc1: 12.2396 (19.4677)  acc5: 32.5521 (37.3920)  time: 14.3994  data: 0.0003  max mem: 52567
Test:  [ 45/131]  eta: 0:20:53  loss: 5.1828 (4.9551)  acc1: 14.5833 (20.0634)  acc5: 32.5521 (38.0491)  time: 14.3992  data: 0.0003  max mem: 52567
Test:  [ 50/131]  eta: 0:19:38  loss: 5.0906 (4.9346)  acc1: 17.1875 (20.1134)  acc5: 34.3750 (38.1996)  time: 14.4003  data: 0.0003  max mem: 52567
Test:  [ 55/131]  eta: 0:18:25  loss: 4.8077 (4.8436)  acc1: 19.5312 (21.2519)  acc5: 37.7604 (39.6856)  time: 14.4014  data: 0.0003  max mem: 52567
Test:  [ 60/131]  eta: 0:17:11  loss: 4.5365 (4.8169)  acc1: 22.3958 (21.5036)  acc5: 41.4062 (39.9804)  time: 14.4015  data: 0.0003  max mem: 52567
Test:  [ 65/131]  eta: 0:15:58  loss: 4.5291 (4.7911)  acc1: 24.4792 (21.6383)  acc5: 42.7083 (40.2778)  time: 14.4019  data: 0.0003  max mem: 52567
Test:  [ 70/131]  eta: 0:14:45  loss: 4.3595 (4.7676)  acc1: 25.5208 (21.7063)  acc5: 45.5729 (40.6653)  time: 14.4000  data: 0.0003  max mem: 52567
Test:  [ 75/131]  eta: 0:13:32  loss: 4.3024 (4.7226)  acc1: 26.0417 (22.4164)  acc5: 45.8333 (41.4885)  time: 14.4008  data: 0.0003  max mem: 52567
Test:  [ 80/131]  eta: 0:12:19  loss: 4.3595 (4.7183)  acc1: 24.2188 (22.4151)  acc5: 45.5729 (41.3934)  time: 14.4016  data: 0.0003  max mem: 52567
Test:  [ 85/131]  eta: 0:11:06  loss: 4.3508 (4.7024)  acc1: 24.2188 (22.4322)  acc5: 45.8333 (41.5425)  time: 14.4000  data: 0.0003  max mem: 52567
Test:  [ 90/131]  eta: 0:09:53  loss: 4.3508 (4.6854)  acc1: 23.6979 (22.6391)  acc5: 46.0938 (41.9443)  time: 14.4008  data: 0.0003  max mem: 52567
Test:  [ 95/131]  eta: 0:08:41  loss: 4.3508 (4.6649)  acc1: 23.6979 (22.9384)  acc5: 44.5312 (42.1332)  time: 14.3995  data: 0.0003  max mem: 52567
Test:  [100/131]  eta: 0:07:28  loss: 4.2911 (4.6425)  acc1: 25.2604 (23.1616)  acc5: 46.0938 (42.4144)  time: 14.4007  data: 0.0003  max mem: 52567
Test:  [105/131]  eta: 0:06:16  loss: 4.2826 (4.6202)  acc1: 29.4271 (23.5014)  acc5: 49.4792 (42.6838)  time: 14.4007  data: 0.0003  max mem: 52567
Test:  [110/131]  eta: 0:05:03  loss: 4.0923 (4.5980)  acc1: 29.4271 (23.7965)  acc5: 47.6562 (42.9734)  time: 14.4000  data: 0.0003  max mem: 52567
Test:  [115/131]  eta: 0:03:51  loss: 4.0974 (4.5950)  acc1: 25.2604 (23.8955)  acc5: 45.3125 (42.9643)  time: 14.3986  data: 0.0002  max mem: 52567
Test:  [120/131]  eta: 0:02:39  loss: 4.2116 (4.5753)  acc1: 25.7812 (24.1305)  acc5: 46.3542 (43.2916)  time: 14.3966  data: 0.0002  max mem: 52567
Test:  [125/131]  eta: 0:01:26  loss: 4.4061 (4.5992)  acc1: 24.7396 (23.9687)  acc5: 43.7500 (42.9688)  time: 14.3962  data: 0.0002  max mem: 52567
Test:  [130/131]  eta: 0:00:14  loss: 4.6808 (4.6053)  acc1: 22.3958 (23.9720)  acc5: 42.5000 (42.9180)  time: 13.8730  data: 0.0001  max mem: 52567
Test: Total time: 0:31:23 (14.3816 s / it)
* Acc@1 23.972 Acc@5 42.918 loss 4.605
Accuracy of the network on the 50000 test images: 24.0%
